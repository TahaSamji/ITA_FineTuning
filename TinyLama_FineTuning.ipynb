{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"829e0f85","cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.113618Z","iopub.execute_input":"2025-06-08T19:38:52.114176Z","iopub.status.idle":"2025-06-08T19:38:52.118020Z","shell.execute_reply.started":"2025-06-08T19:38:52.114156Z","shell.execute_reply":"2025-06-08T19:38:52.117453Z"}},"outputs":[],"execution_count":22},{"id":"4ef75cd9","cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.157013Z","iopub.execute_input":"2025-06-08T19:38:52.157458Z","iopub.status.idle":"2025-06-08T19:38:52.160779Z","shell.execute_reply.started":"2025-06-08T19:38:52.157442Z","shell.execute_reply":"2025-06-08T19:38:52.160277Z"}},"outputs":[],"execution_count":23},{"id":"ad49ad5f","cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.183206Z","iopub.execute_input":"2025-06-08T19:38:52.183588Z","iopub.status.idle":"2025-06-08T19:38:52.187650Z","shell.execute_reply.started":"2025-06-08T19:38:52.183574Z","shell.execute_reply":"2025-06-08T19:38:52.187149Z"}},"outputs":[],"execution_count":24},{"id":"f5b6efd2","cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.218775Z","iopub.execute_input":"2025-06-08T19:38:52.219196Z","iopub.status.idle":"2025-06-08T19:38:52.223169Z","shell.execute_reply.started":"2025-06-08T19:38:52.219181Z","shell.execute_reply":"2025-06-08T19:38:52.222442Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":25},{"id":"bcee66dc","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.266169Z","iopub.execute_input":"2025-06-08T19:38:52.266347Z","iopub.status.idle":"2025-06-08T19:38:52.487582Z","shell.execute_reply.started":"2025-06-08T19:38:52.266334Z","shell.execute_reply":"2025-06-08T19:38:52.487049Z"}},"outputs":[],"execution_count":26},{"id":"a8d7cdff-5596-4b28-a2dc-d5e0e6e45d95","cell_type":"code","source":"# # Load dataset and tokenizer\n# dataset2 = load_dataset(\"allenai/soda\")\n\n\n\n# def tokenize_function(examples):\n#     # Concatenate dialog turns into a single string for language modeling\n#     texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n#     return tokenizer(texts, truncation=True, max_length=512)\n\n# # Tokenize datasets\n# tokenized_datasets = dataset2.map(tokenize_function, batched=True, remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'])\n# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n# small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.488701Z","iopub.execute_input":"2025-06-08T19:38:52.488924Z","iopub.status.idle":"2025-06-08T19:38:52.492251Z","shell.execute_reply.started":"2025-06-08T19:38:52.488908Z","shell.execute_reply":"2025-06-08T19:38:52.491727Z"}},"outputs":[],"execution_count":27},{"id":"be943d01-c616-49f3-999c-7f90c18bd511","cell_type":"code","source":"print(dataset2)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.492845Z","iopub.execute_input":"2025-06-08T19:38:52.493057Z","iopub.status.idle":"2025-06-08T19:38:52.515122Z","shell.execute_reply.started":"2025-06-08T19:38:52.493021Z","shell.execute_reply":"2025-06-08T19:38:52.514576Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n})\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 148968\n    })\n})"},"metadata":{}}],"execution_count":28},{"id":"f24655c4","cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=4,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.516488Z","iopub.execute_input":"2025-06-08T19:38:52.516673Z","iopub.status.idle":"2025-06-08T19:38:52.533159Z","shell.execute_reply.started":"2025-06-08T19:38:52.516659Z","shell.execute_reply":"2025-06-08T19:38:52.532468Z"}},"outputs":[],"execution_count":29},{"id":"f418c726","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:52.533780Z","iopub.execute_input":"2025-06-08T19:38:52.533970Z","iopub.status.idle":"2025-06-08T19:38:53.501786Z","shell.execute_reply.started":"2025-06-08T19:38:52.533952Z","shell.execute_reply":"2025-06-08T19:38:53.501102Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n[GPU] Memory Allocated: 10.27 GB, Cached: 11.01 GB\n[CPU] Memory Usage: 3.88 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":30},{"id":"0863d39d","cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=1000,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:53.502589Z","iopub.execute_input":"2025-06-08T19:38:53.502829Z","iopub.status.idle":"2025-06-08T19:38:53.541475Z","shell.execute_reply.started":"2025-06-08T19:38:53.502812Z","shell.execute_reply":"2025-06-08T19:38:53.540755Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 10.27 GB, Cached: 11.01 GB\n[CPU] Memory Usage: 3.89 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":31},{"id":"0355f85f","cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:53.542272Z","iopub.execute_input":"2025-06-08T19:38:53.542528Z","iopub.status.idle":"2025-06-08T19:38:53.571313Z","shell.execute_reply.started":"2025-06-08T19:38:53.542507Z","shell.execute_reply":"2025-06-08T19:38:53.570804Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":32},{"id":"5daa30a3","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\nmodel_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n             torch_dtype=torch.bfloat16).to(device)\n\ninput_text_before = \"Hello, how are you?\"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:53.572271Z","iopub.execute_input":"2025-06-08T19:38:53.572523Z","iopub.status.idle":"2025-06-08T19:38:55.973347Z","shell.execute_reply.started":"2025-06-08T19:38:53.572501Z","shell.execute_reply":"2025-06-08T19:38:55.972739Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: Hello, how are you?\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Response (Before): Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again,\n---\n","output_type":"stream"}],"execution_count":33},{"id":"b21b2ae4","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\n#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\ninput_text_before = \"The USA celebrate independence day on \"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:55.974296Z","iopub.execute_input":"2025-06-08T19:38:55.974567Z","iopub.status.idle":"2025-06-08T19:38:57.214252Z","shell.execute_reply.started":"2025-06-08T19:38:55.974542Z","shell.execute_reply":"2025-06-08T19:38:57.213487Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: The USA celebrate independence day on \nGenerated Response (Before): The USA celebrate independence day on 4th July.\n\n2. The USA celebrate independence day on 4th July.\n\n3. The USA celebrate independence day on 4th July.\n\n4. The\n---\n","output_type":"stream"}],"execution_count":34},{"id":"9cb4c8f9","cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:57.216207Z","iopub.execute_input":"2025-06-08T19:38:57.216414Z","iopub.status.idle":"2025-06-08T19:38:57.220914Z","shell.execute_reply.started":"2025-06-08T19:38:57.216398Z","shell.execute_reply":"2025-06-08T19:38:57.220343Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":35},{"id":"16e84bb6","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:38:57.221556Z","iopub.execute_input":"2025-06-08T19:38:57.221812Z","iopub.status.idle":"2025-06-08T20:09:23.685271Z","shell.execute_reply.started":"2025-06-08T19:38:57.221787Z","shell.execute_reply":"2025-06-08T20:09:23.684661Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 30:24, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.737500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1500, training_loss=0.7328169759114583, metrics={'train_runtime': 1826.0045, 'train_samples_per_second': 3.286, 'train_steps_per_second': 0.821, 'total_flos': 9440523922636800.0, 'train_loss': 0.7328169759114583, 'epoch': 3.0})"},"metadata":{}}],"execution_count":36},{"id":"17b74a82-ad5d-42cf-afe1-cd84034ecabb","cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T20:09:23.685942Z","iopub.execute_input":"2025-06-08T20:09:23.686153Z","iopub.status.idle":"2025-06-08T20:09:23.975349Z","shell.execute_reply.started":"2025-06-08T20:09:23.686138Z","shell.execute_reply":"2025-06-08T20:09:23.974558Z"}},"outputs":[],"execution_count":37},{"id":"13a12b7c-3d5a-455e-a483-1fbc1c954668","cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T20:09:23.976133Z","iopub.execute_input":"2025-06-08T20:09:23.976614Z","iopub.status.idle":"2025-06-08T20:09:23.981072Z","shell.execute_reply.started":"2025-06-08T20:09:23.976590Z","shell.execute_reply":"2025-06-08T20:09:23.980249Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 8.22 GB, Cached: 13.81 GB\n[CPU] Memory Usage: 3.88 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":38},{"id":"98331efb-c1a1-4223-b183-5731ddbddd78","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_text = \"Hello, how are you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")\n\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T20:09:23.981744Z","iopub.execute_input":"2025-06-08T20:09:23.981994Z","iopub.status.idle":"2025-06-08T20:09:31.393993Z","shell.execute_reply.started":"2025-06-08T20:09:23.981980Z","shell.execute_reply":"2025-06-08T20:09:31.393237Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nGenerating response:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response: Hello, how are you? I'm doing well. I'm just getting started with my new job. That sounds great! I'm excited to start working and see how it goes. I'm sure it will be great. I'm sure it will be. I'm looking forward to it too. I'm excited to see what the future holds for me. I'm excited too. I'm looking forward to seeing how my new job will impact my life. I'm looking forward to seeing how it will impact my life too. I'm excited to see how my new job will impact my life. I'm excited to see how it will impact my life too. I'm excited\n[GPU] Memory Allocated: 8.22 GB, Cached: 12.75 GB\n[CPU] Memory Usage: 4.51 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":39},{"id":"e8dbebe8-310c-420d-a21e-95c3ea707bb2","cell_type":"code","source":"input_text = \"The USA celebrate independence day on \"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T20:09:31.394824Z","iopub.execute_input":"2025-06-08T20:09:31.395091Z","iopub.status.idle":"2025-06-08T20:09:33.968419Z","shell.execute_reply.started":"2025-06-08T20:09:31.395069Z","shell.execute_reply":"2025-06-08T20:09:33.967582Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: The USA celebrate independence day on \nGenerated Response: The USA celebrate independence day on 4th July.\n\n2. India Independence Day is celebrated on 15th August.\n\n3. Canada celebrates its independence day on 1st October.\n\n4. Australia celebrates its independence day on 26th June.\n\n5. New Zealand celebrates its independence day on 5th October.\n\nThese are just a few examples, but there are many more countries that celebrate independence day on different dates.\n","output_type":"stream"}],"execution_count":40},{"id":"6281e296-a328-4f43-a4fa-67ff1c06c6b9","cell_type":"code","source":"input_text = \"Explain how gravity works?. Can you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=250,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T20:09:33.969288Z","iopub.execute_input":"2025-06-08T20:09:33.970113Z","iopub.status.idle":"2025-06-08T20:09:40.445538Z","shell.execute_reply.started":"2025-06-08T20:09:33.970093Z","shell.execute_reply":"2025-06-08T20:09:40.444839Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: Explain how gravity works?. Can you?\nGenerated Response: Explain how gravity works?. Can you?\nGravity is the force that holds the Earth and all other objects in the universe together. It is a force that is present in all objects in the universe, regardless of their size or shape. Gravity is a result of the attraction between two objects that are moving towards each other at a constant speed. The force of gravity is proportional to the masses of the two objects and inversely proportional to the square of the distance between them.\nGravity is a fundamental force of nature that is present in all objects in the universe. It is a result of the attraction between two objects that are moving towards each other at a constant speed. The force of gravity is proportional to the masses of the two objects and inversely proportional to the square of the distance between them.\nGravity is a force that is present in all objects in the universe, regardless of their size or shape. It is a result of the attraction between two objects that are moving towards each other at a constant speed. The force of gravity is proportional to the masses of the two objects and inversely proportional to the square of the distance between them.\nGravity is a force\n","output_type":"stream"}],"execution_count":41}]}