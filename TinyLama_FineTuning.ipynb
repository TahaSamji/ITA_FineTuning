{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"829e0f85","cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:22.345361Z","iopub.execute_input":"2025-06-08T17:25:22.345590Z","iopub.status.idle":"2025-06-08T17:25:50.628836Z","shell.execute_reply.started":"2025-06-08T17:25:22.345567Z","shell.execute_reply":"2025-06-08T17:25:50.628030Z"}},"outputs":[{"name":"stderr","text":"2025-06-08 17:25:37.018149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749403537.202680      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749403537.257952      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"4ef75cd9","cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:50.629591Z","iopub.execute_input":"2025-06-08T17:25:50.630143Z","iopub.status.idle":"2025-06-08T17:25:50.784971Z","shell.execute_reply.started":"2025-06-08T17:25:50.630123Z","shell.execute_reply":"2025-06-08T17:25:50.784057Z"}},"outputs":[],"execution_count":2},{"id":"ad49ad5f","cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:50.786927Z","iopub.execute_input":"2025-06-08T17:25:50.787149Z","iopub.status.idle":"2025-06-08T17:25:50.806664Z","shell.execute_reply.started":"2025-06-08T17:25:50.787132Z","shell.execute_reply":"2025-06-08T17:25:50.806083Z"}},"outputs":[],"execution_count":3},{"id":"f5b6efd2","cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:50.807429Z","iopub.execute_input":"2025-06-08T17:25:50.807675Z","iopub.status.idle":"2025-06-08T17:25:50.819200Z","shell.execute_reply.started":"2025-06-08T17:25:50.807648Z","shell.execute_reply":"2025-06-08T17:25:50.818553Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":4},{"id":"bcee66dc","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:50.819929Z","iopub.execute_input":"2025-06-08T17:25:50.820291Z","iopub.status.idle":"2025-06-08T17:25:54.287070Z","shell.execute_reply.started":"2025-06-08T17:25:50.820267Z","shell.execute_reply":"2025-06-08T17:25:54.286484Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7bb29163284292ae51f9b85bfebc50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3719deab4b4590ab26bf3b3c81cc5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c23d3b00b5044d8a60f8299c3b05701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2328e036c50944678b604d2a6410d9b2"}},"metadata":{}}],"execution_count":5},{"id":"a8d7cdff-5596-4b28-a2dc-d5e0e6e45d95","cell_type":"code","source":"# # Load dataset and tokenizer\n# dataset2 = load_dataset(\"daily_dialog\")\n\n\n\n# def tokenize_function(examples):\n#     # Concatenate dialog turns into a single string for language modeling\n#     texts = [\" \".join(dialog) for dialog in examples[\"dialog\"]]\n#     return tokenizer(texts, truncation=True, max_length=512)\n\n# # Tokenize datasets\n# tokenized_datasets = dataset2.map(tokenize_function, batched=True, remove_columns=[\"dialog\", \"act\", \"emotion\"])\n# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\n# small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:54.287846Z","iopub.execute_input":"2025-06-08T17:25:54.288078Z","iopub.status.idle":"2025-06-08T17:25:54.291639Z","shell.execute_reply.started":"2025-06-08T17:25:54.288061Z","shell.execute_reply":"2025-06-08T17:25:54.290955Z"}},"outputs":[],"execution_count":6},{"id":"e00840a9-40f0-4a6e-84a6-d1dbb5291550","cell_type":"code","source":"\n# print(dataset2)\n# tokenized_datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:54.292355Z","iopub.execute_input":"2025-06-08T17:25:54.292650Z","iopub.status.idle":"2025-06-08T17:25:54.316046Z","shell.execute_reply.started":"2025-06-08T17:25:54.292626Z","shell.execute_reply":"2025-06-08T17:25:54.315412Z"}},"outputs":[],"execution_count":7},{"id":"f1a556d4-583f-4892-aa43-448067c77193","cell_type":"code","source":"dataset = load_dataset(\"knkarthick/dialogsum\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:54.316666Z","iopub.execute_input":"2025-06-08T17:25:54.317006Z","iopub.status.idle":"2025-06-08T17:25:59.924120Z","shell.execute_reply.started":"2025-06-08T17:25:54.316987Z","shell.execute_reply":"2025-06-08T17:25:59.923627Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9a785768d347deb7a04e295db31085"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f271a98b156c47b99b1b6d9accc8a401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3973c8665ab4b038fa66da02a52f796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805af88162b5432d8df87fb421e028b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f337821842444d49ac24603103673b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f18511034abb40f4ba8f6d845413ca86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bcd9c369f344e7d87239b2e46284688"}},"metadata":{}}],"execution_count":8},{"id":"27738da1","cell_type":"code","source":"def tokenize_function(examples):\n    # Concatenate dialog turns into a single string for language modeling\n    texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n    return tokenizer(texts, truncation=True, max_length=512)\n\n\n# Tokenize datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['id', 'dialogue', 'summary', 'topic'])\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:25:59.926072Z","iopub.execute_input":"2025-06-08T17:25:59.926665Z","iopub.status.idle":"2025-06-08T17:26:08.969308Z","shell.execute_reply.started":"2025-06-08T17:25:59.926646Z","shell.execute_reply":"2025-06-08T17:26:08.968447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"834ce96911d84b0eaf5b99a0fadb122c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2890d428d5c7472c8ef268de95304f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db06597a35c84ec38bb8a7b7eeb08193"}},"metadata":{}}],"execution_count":9},{"id":"be943d01-c616-49f3-999c-7f90c18bd511","cell_type":"code","source":"print(dataset)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:26:08.970230Z","iopub.execute_input":"2025-06-08T17:26:08.970545Z","iopub.status.idle":"2025-06-08T17:26:08.976345Z","shell.execute_reply.started":"2025-06-08T17:26:08.970518Z","shell.execute_reply":"2025-06-08T17:26:08.975492Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}],"execution_count":10},{"id":"d3032cbc","cell_type":"code","source":"dataset[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:26:08.977141Z","iopub.execute_input":"2025-06-08T17:26:08.977919Z","iopub.status.idle":"2025-06-08T17:26:09.133402Z","shell.execute_reply.started":"2025-06-08T17:26:08.977888Z","shell.execute_reply":"2025-06-08T17:26:09.132830Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'dialogue', 'summary', 'topic'],\n    num_rows: 12460\n})"},"metadata":{}}],"execution_count":11},{"id":"f24655c4","cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:26:09.134061Z","iopub.execute_input":"2025-06-08T17:26:09.134314Z","iopub.status.idle":"2025-06-08T17:26:09.145084Z","shell.execute_reply.started":"2025-06-08T17:26:09.134296Z","shell.execute_reply":"2025-06-08T17:26:09.144473Z"}},"outputs":[],"execution_count":12},{"id":"f418c726","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:26:09.145741Z","iopub.execute_input":"2025-06-08T17:26:09.145984Z","iopub.status.idle":"2025-06-08T17:27:09.402077Z","shell.execute_reply.started":"2025-06-08T17:26:09.145969Z","shell.execute_reply":"2025-06-08T17:27:09.401356Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14db95be60b54c6e8938c3c976f6ef81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78d14039f9c848d59daca662b839ac2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5338c84ddf52420d8944c4e10256ba7c"}},"metadata":{}},{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 1.80 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":13},{"id":"0863d39d","cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=1000,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:09.402755Z","iopub.execute_input":"2025-06-08T17:27:09.403021Z","iopub.status.idle":"2025-06-08T17:27:09.439634Z","shell.execute_reply.started":"2025-06-08T17:27:09.403004Z","shell.execute_reply":"2025-06-08T17:27:09.438963Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 1.81 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":14},{"id":"0355f85f","cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:09.440365Z","iopub.execute_input":"2025-06-08T17:27:09.440546Z","iopub.status.idle":"2025-06-08T17:27:09.606032Z","shell.execute_reply.started":"2025-06-08T17:27:09.440527Z","shell.execute_reply":"2025-06-08T17:27:09.605514Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":15},{"id":"5daa30a3","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\nmodel_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n             torch_dtype=torch.bfloat16).to(device)\n\ninput_text_before = \"Hello, how are you?\"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:09.606680Z","iopub.execute_input":"2025-06-08T17:27:09.606890Z","iopub.status.idle":"2025-06-08T17:27:13.089744Z","shell.execute_reply.started":"2025-06-08T17:27:09.606874Z","shell.execute_reply":"2025-06-08T17:27:13.088981Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response (Before): Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again,\n---\n","output_type":"stream"}],"execution_count":16},{"id":"b21b2ae4","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\n#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\ninput_text_before = \"The USA celebrate independence day on \"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:13.090717Z","iopub.execute_input":"2025-06-08T17:27:13.090997Z","iopub.status.idle":"2025-06-08T17:27:14.396797Z","shell.execute_reply.started":"2025-06-08T17:27:13.090974Z","shell.execute_reply":"2025-06-08T17:27:14.395994Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: The USA celebrate independence day on \nGenerated Response (Before): The USA celebrate independence day on 4th July.\n\n2. The USA celebrate independence day on 4th July.\n\n3. The USA celebrate independence day on 4th July.\n\n4. The\n---\n","output_type":"stream"}],"execution_count":17},{"id":"9cb4c8f9","cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:14.397573Z","iopub.execute_input":"2025-06-08T17:27:14.397942Z","iopub.status.idle":"2025-06-08T17:27:14.403516Z","shell.execute_reply.started":"2025-06-08T17:27:14.397917Z","shell.execute_reply":"2025-06-08T17:27:14.402874Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":18},{"id":"16e84bb6","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:27:14.404326Z","iopub.execute_input":"2025-06-08T17:27:14.404581Z","iopub.status.idle":"2025-06-08T17:48:58.888671Z","shell.execute_reply.started":"2025-06-08T17:27:14.404557Z","shell.execute_reply":"2025-06-08T17:48:58.888048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 21:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.5579511108398437, metrics={'train_runtime': 1303.9512, 'train_samples_per_second': 1.534, 'train_steps_per_second': 0.383, 'total_flos': 6355636593573888.0, 'train_loss': 0.5579511108398437, 'epoch': 1.0})"},"metadata":{}}],"execution_count":19},{"id":"17b74a82-ad5d-42cf-afe1-cd84034ecabb","cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:51:58.152036Z","iopub.execute_input":"2025-06-08T17:51:58.152330Z","iopub.status.idle":"2025-06-08T17:51:58.829885Z","shell.execute_reply.started":"2025-06-08T17:51:58.152301Z","shell.execute_reply":"2025-06-08T17:51:58.829115Z"}},"outputs":[],"execution_count":20},{"id":"13a12b7c-3d5a-455e-a483-1fbc1c954668","cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:52:11.565835Z","iopub.execute_input":"2025-06-08T17:52:11.566100Z","iopub.status.idle":"2025-06-08T17:52:11.571205Z","shell.execute_reply.started":"2025-06-08T17:52:11.566080Z","shell.execute_reply":"2025-06-08T17:52:11.570621Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 4.12 GB, Cached: 8.11 GB\n[CPU] Memory Usage: 2.63 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":21},{"id":"98331efb-c1a1-4223-b183-5731ddbddd78","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_text = \"Hello, how are you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")\n\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T18:35:12.389522Z","iopub.execute_input":"2025-06-08T18:35:12.389901Z","iopub.status.idle":"2025-06-08T18:35:39.877499Z","shell.execute_reply.started":"2025-06-08T18:35:12.389870Z","shell.execute_reply":"2025-06-08T18:35:39.876842Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nGenerating response:\nInput: Hello, how are you?\nGenerated Response: Hello, how are you?\n\nJane:\n(smiling)\nI'm doing well, thank you.\n\nTom:\n(smiling back)\nMe too.\n\nJane:\n(smiling)\nI'm glad to hear that.\n\nTom:\n(smiling)\nMe too.\n\nJane:\n(smiling)\nI'm glad we're finally meeting.\n\nTom:\n(smiling)\nMe too.\n\nJane:\n(smiling)\nSo, what brings you to our city?\n\nTom:\n(smiling)\nI'm here to visit my sister.\n\nJane\n[GPU] Memory Allocated: 8.22 GB, Cached: 12.88 GB\n[CPU] Memory Usage: 6.28 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":30},{"id":"e8dbebe8-310c-420d-a21e-95c3ea707bb2","cell_type":"code","source":"input_text = \"The USA celebrate independence day on \"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T17:54:29.933528Z","iopub.execute_input":"2025-06-08T17:54:29.934126Z","iopub.status.idle":"2025-06-08T17:54:34.012892Z","shell.execute_reply.started":"2025-06-08T17:54:29.934103Z","shell.execute_reply":"2025-06-08T17:54:34.012153Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: The USA celebrate independence day on \nGenerated Response: The USA celebrate independence day on 4th July.\n\n4. The USA celebrate independence day on 4th July.\n\n5. The USA celebrate independence day on 4th July.\n\n6. The USA celebrate independence day on 4th July.\n\n7. The USA celebrate independence day on 4th July.\n\n8. The USA celebrate independence day on 4th July.\n\n9. The USA celebrate independence day on 4th July.\n\n10. The USA celebrate independence day on 4th July.\n\n11. The USA celebrate independence day on 4th July.\n\n12. The USA\n","output_type":"stream"}],"execution_count":23},{"id":"6281e296-a328-4f43-a4fa-67ff1c06c6b9","cell_type":"code","source":"input_text = \"Explain how gravity works. Can you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=250,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T18:33:25.576307Z","iopub.execute_input":"2025-06-08T18:33:25.576576Z","iopub.status.idle":"2025-06-08T18:33:25.638293Z","shell.execute_reply.started":"2025-06-08T18:33:25.576556Z","shell.execute_reply":"2025-06-08T18:33:25.637719Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: Explain how gravity works. Can you?\nGenerated Response: Explain how gravity works. Can you?\n","output_type":"stream"}],"execution_count":27}]}