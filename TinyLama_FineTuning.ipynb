{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:13:26.925848Z","iopub.execute_input":"2025-06-09T18:13:26.926105Z","iopub.status.idle":"2025-06-09T18:13:56.879704Z","shell.execute_reply.started":"2025-06-09T18:13:26.926085Z","shell.execute_reply":"2025-06-09T18:13:56.878871Z"}},"outputs":[{"name":"stderr","text":"2025-06-09 18:13:41.593449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749492821.774848      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749492821.828390      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:13:56.881187Z","iopub.execute_input":"2025-06-09T18:13:56.881925Z","iopub.status.idle":"2025-06-09T18:15:17.981846Z","shell.execute_reply.started":"2025-06-09T18:13:56.881903Z","shell.execute_reply":"2025-06-09T18:15:17.980709Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nRequirement already satisfied: transformers>=4.50.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.3)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (0.21.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.18.1-py3-none-any.whl (366 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.18.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:15:17.983338Z","iopub.execute_input":"2025-06-09T18:15:17.983700Z","iopub.status.idle":"2025-06-09T18:15:18.180071Z","shell.execute_reply.started":"2025-06-09T18:15:17.983656Z","shell.execute_reply":"2025-06-09T18:15:18.179120Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:15:18.181885Z","iopub.execute_input":"2025-06-09T18:15:18.182151Z","iopub.status.idle":"2025-06-09T18:15:18.194037Z","shell.execute_reply.started":"2025-06-09T18:15:18.182132Z","shell.execute_reply":"2025-06-09T18:15:18.193191Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:15:18.195000Z","iopub.execute_input":"2025-06-09T18:15:18.195367Z","iopub.status.idle":"2025-06-09T18:15:18.207083Z","shell.execute_reply.started":"2025-06-09T18:15:18.195348Z","shell.execute_reply":"2025-06-09T18:15:18.206275Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:15:18.207901Z","iopub.execute_input":"2025-06-09T18:15:18.208111Z","iopub.status.idle":"2025-06-09T18:15:19.504580Z","shell.execute_reply.started":"2025-06-09T18:15:18.208096Z","shell.execute_reply":"2025-06-09T18:15:19.503719Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0c48f1a22e40c39c017ab672d7d790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdf791ee4d6436ba3a5512b73cbf4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dda5314feac4402a87d24a6d392ad687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"621340adc56744538d68a6b246281afa"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Load dataset and tokenizer\ndataset = load_dataset(\"allenai/soda\")\n\n\n\ndef tokenize_function(examples):\n    # Concatenate dialog turns into a single string for language modeling\n    texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n    return tokenizer(texts, truncation=True, max_length=512)\n\n# Tokenize datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'])\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:15:19.505401Z","iopub.execute_input":"2025-06-09T18:15:19.505618Z","iopub.status.idle":"2025-06-09T18:22:00.628636Z","shell.execute_reply.started":"2025-06-09T18:15:19.505601Z","shell.execute_reply":"2025-06-09T18:22:00.627983Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d28325902c406587307309201c4202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/689M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3353b922c64168b6e9042445c13d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.parquet:   0%|          | 0.00/82.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285874b1163742ad9bcf4c5bc00a6490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8660bb69937a45cc978411b2b2232ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e8f863c2564f85a04d67cdf0d0a80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84704e63a1844b59b0497eef52868975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df229b1dc1834fbebe02ed35ee611784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4095274ba7473bbbbd55fced6efe59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2856338454ba4a9e966c26e5fcd0d353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb42c511df084b1d9541da17a48e21b6"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(dataset)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:00.629604Z","iopub.execute_input":"2025-06-09T18:22:00.629884Z","iopub.status.idle":"2025-06-09T18:22:00.636029Z","shell.execute_reply.started":"2025-06-09T18:22:00.629862Z","shell.execute_reply":"2025-06-09T18:22:00.635170Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n})\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 148968\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:00.637067Z","iopub.execute_input":"2025-06-09T18:22:00.637369Z","iopub.status.idle":"2025-06-09T18:22:00.675282Z","shell.execute_reply.started":"2025-06-09T18:22:00.637345Z","shell.execute_reply":"2025-06-09T18:22:00.674297Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:00.678666Z","iopub.execute_input":"2025-06-09T18:22:00.678999Z","iopub.status.idle":"2025-06-09T18:22:19.659808Z","shell.execute_reply.started":"2025-06-09T18:22:00.678977Z","shell.execute_reply":"2025-06-09T18:22:19.658857Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0993f839dd411db8db623f8235fd7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d423acb1174c52a5ec9acd3b543f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c395713516a1457480f5a8a0a1ecc9c1"}},"metadata":{}},{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.38 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:19.660809Z","iopub.execute_input":"2025-06-09T18:22:19.661173Z","iopub.status.idle":"2025-06-09T18:22:19.709971Z","shell.execute_reply.started":"2025-06-09T18:22:19.661152Z","shell.execute_reply":"2025-06-09T18:22:19.709115Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.39 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:19.710897Z","iopub.execute_input":"2025-06-09T18:22:19.711185Z","iopub.status.idle":"2025-06-09T18:22:19.916937Z","shell.execute_reply.started":"2025-06-09T18:22:19.711162Z","shell.execute_reply":"2025-06-09T18:22:19.916127Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\nmodel_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n             torch_dtype=torch.bfloat16).to(device)\n\ninput_text_before = \"Hello, how are you?\"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:19.917885Z","iopub.execute_input":"2025-06-09T18:22:19.918181Z","iopub.status.idle":"2025-06-09T18:22:23.882450Z","shell.execute_reply.started":"2025-06-09T18:22:19.918160Z","shell.execute_reply":"2025-06-09T18:22:23.881393Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response (Before): Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again,\n---\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\n#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\ninput_text_before = \"The USA celebrate independence day on \"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:23.883385Z","iopub.execute_input":"2025-06-09T18:22:23.883937Z","iopub.status.idle":"2025-06-09T18:22:25.399543Z","shell.execute_reply.started":"2025-06-09T18:22:23.883909Z","shell.execute_reply":"2025-06-09T18:22:25.398625Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: The USA celebrate independence day on \nGenerated Response (Before): The USA celebrate independence day on 4th July.\n\n2. The USA celebrate independence day on 4th July.\n\n3. The USA celebrate independence day on 4th July.\n\n4. The\n---\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:25.400533Z","iopub.execute_input":"2025-06-09T18:22:25.400881Z","iopub.status.idle":"2025-06-09T18:22:32.293555Z","shell.execute_reply.started":"2025-06-09T18:22:25.400854Z","shell.execute_reply":"2025-06-09T18:22:32.292621Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:32.294502Z","iopub.execute_input":"2025-06-09T18:22:32.294785Z","iopub.status.idle":"2025-06-09T18:22:59.089067Z","shell.execute_reply.started":"2025-06-09T18:22:32.294739Z","shell.execute_reply":"2025-06-09T18:22:59.088225Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:24, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.784100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.763500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.938800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.844500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.828700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=50, training_loss=0.8319242572784424, metrics={'train_runtime': 26.3385, 'train_samples_per_second': 3.797, 'train_steps_per_second': 1.898, 'total_flos': 135250878947328.0, 'train_loss': 0.8319242572784424, 'epoch': 1.0})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:59.090081Z","iopub.execute_input":"2025-06-09T18:22:59.090351Z","iopub.status.idle":"2025-06-09T18:22:59.269240Z","shell.execute_reply.started":"2025-06-09T18:22:59.090333Z","shell.execute_reply":"2025-06-09T18:22:59.268314Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:59.270131Z","iopub.execute_input":"2025-06-09T18:22:59.270337Z","iopub.status.idle":"2025-06-09T18:22:59.275460Z","shell.execute_reply.started":"2025-06-09T18:22:59.270323Z","shell.execute_reply":"2025-06-09T18:22:59.274795Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 4.12 GB, Cached: 7.52 GB\n[CPU] Memory Usage: 3.14 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_text = \"Hello, how are you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")\n\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:22:59.276299Z","iopub.execute_input":"2025-06-09T18:22:59.276496Z","iopub.status.idle":"2025-06-09T18:23:07.721155Z","shell.execute_reply.started":"2025-06-09T18:22:59.276481Z","shell.execute_reply":"2025-06-09T18:23:07.720408Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nGenerating response:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response: Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please?\nCan you repeat that again, please\n[GPU] Memory Allocated: 8.22 GB, Cached: 10.54 GB\n[CPU] Memory Usage: 3.93 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"input_text = \"The USA celebrate independence day on \"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:07.722097Z","iopub.execute_input":"2025-06-09T18:23:07.722406Z","iopub.status.idle":"2025-06-09T18:23:12.008430Z","shell.execute_reply.started":"2025-06-09T18:23:07.722380Z","shell.execute_reply":"2025-06-09T18:23:12.007494Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: The USA celebrate independence day on \nGenerated Response: The USA celebrate independence day on 4th July.\n\n4. The USA celebrate independence day on 4th July.\n\n5. The USA celebrate independence day on 4th July.\n\n6. The USA celebrate independence day on 4th July.\n\n7. The USA celebrate independence day on 4th July.\n\n8. The USA celebrate independence day on 4th July.\n\n9. The USA celebrate independence day on 4th July.\n\n10. The USA celebrate independence day on 4th July.\n\n11. The USA celebrate independence day on 4th July.\n\n12. The USA\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"input_text = \"So light is made up of colors?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:12.009441Z","iopub.execute_input":"2025-06-09T18:23:12.009819Z","iopub.status.idle":"2025-06-09T18:23:12.050257Z","shell.execute_reply.started":"2025-06-09T18:23:12.009794Z","shell.execute_reply":"2025-06-09T18:23:12.049521Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: So light is made up of colors?\nGenerated Response: So light is made up of colors?\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"small_eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:12.050959Z","iopub.execute_input":"2025-06-09T18:23:12.051175Z","iopub.status.idle":"2025-06-09T18:23:12.056634Z","shell.execute_reply.started":"2025-06-09T18:23:12.051159Z","shell.execute_reply":"2025-06-09T18:23:12.055963Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\n\n# Tokenize the generated response\ncandidate = generated_response.split()\n\n# Define reference responses (tokenized)\nreferences = [\n    \"Yes, that's right. Light is made up of different colors, and when those colors mix together, we see white light. But light can also bend and bounce off surfaces. How does it do that? When light hits a surface, it can either reflect off of the surface or refract through the surface. Reflection is when the light bounces off of the surface, and refraction is when the light bends as it passes through the surface. So when light reflects off of a mirror, it's because the light is bouncing off of the surface of the mirror? That's right. Mirrors are very good at reflecting light because they have a smooth, flat surface. And when light refracts through something like a prism, it's because the light is bending as it passes through the surface of the prism? That's right. When light passes through a prism, it bends because the Prism bends the light.\"\n]\n\n# Tokenize each reference\nreference_tokens = [ref.split() for ref in references]\n\n# Calculate BLEU scores\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmoothie = SmoothingFunction().method4  # Helps with zero scores for higher n-grams\n\nprint('Bleu 4: %f' % sentence_bleu(reference_tokens, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:12.057740Z","iopub.execute_input":"2025-06-09T18:23:12.058035Z","iopub.status.idle":"2025-06-09T18:23:12.556051Z","shell.execute_reply.started":"2025-06-09T18:23:12.058017Z","shell.execute_reply":"2025-06-09T18:23:12.555369Z"}},"outputs":[{"name":"stdout","text":"Bleu 4: 0.000000\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nsmoothie = SmoothingFunction().method4\n\ninput_texts = [\n    \"So light is made up of colors?\",\n    \"Why is the sky blue?\",\n    \"How does gravity work?\",\n    \"What causes rainbows?\",\n    \"Why do we see lightning before thunder?\",\n    \"How do plants make food?\",\n    \"Why do we sleep?\",\n    \"What is photosynthesis?\",\n    \"How does the internet work?\",\n    \"Why do birds migrate?\"\n]\n\nreference_answers = [\n    \"Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\",\n    \"The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\",\n    \"Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\",\n    \"Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\",\n    \"Light travels faster than sound, so we see lightning before we hear thunder.\",\n    \"Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\",\n    \"We sleep to allow the body and brain to rest, recover, and process information from the day.\",\n    \"Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\",\n    \"The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\",\n    \"Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\"\n]\n\nbleu_scores = []\n\nfor i in range(10):\n    input_text = input_texts[i]\n    reference = reference_answers[i]\n\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    output = model_for_generation.generate(\n        input_ids,\n        max_length=150,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    candidate = generated_response.split()\n    reference_tokens = [reference.split()]\n\n    score = sentence_bleu(reference_tokens, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n    bleu_scores.append(score)\n\n    print(f\"\\nPrompt {i+1}: {input_text}\")\n    print(f\"Generated: {generated_response}\")\n    print(f\"Reference: {reference}\")\n    print(f\"BLEU-4 Score: {score:.4f}\")\n\nprint(\"\\nAverage BLEU-4 Score across all prompts: %.4f\" % (sum(bleu_scores) / len(bleu_scores)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:12.556834Z","iopub.execute_input":"2025-06-09T18:23:12.557130Z","iopub.status.idle":"2025-06-09T18:23:30.396540Z","shell.execute_reply.started":"2025-06-09T18:23:12.557112Z","shell.execute_reply":"2025-06-09T18:23:30.395799Z"}},"outputs":[{"name":"stdout","text":"\nPrompt 1: So light is made up of colors?\nGenerated: So light is made up of colors?\nReference: Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\nBLEU-4 Score: 0.0721\n\nPrompt 2: Why is the sky blue?\nGenerated: Why is the sky blue?\n\n1. The sun is the primary source of light in the sky.\n\n2. The Earth is tilted on its axis, which causes the sun to appear to rise in the east and set in the west.\n\n3. The Earth's rotation around the sun causes the seasons to occur.\n\n4. The Earth's rotation causes the tilt of the Earth's axis, which causes the sun to appear to move across the sky.\n\n5. The Earth's rotation causes the Earth's axis to tilt towards the sun, which causes the seasons to occur.\n\n6. The Earth's rotation causes the Earth's axis to t\nReference: The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\nBLEU-4 Score: 0.0043\n\nPrompt 3: How does gravity work?\nGenerated: How does gravity work?\nReference: Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\nBLEU-4 Score: 0.0000\n\nPrompt 4: What causes rainbows?\nGenerated: What causes rainbows?\nReference: Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\nBLEU-4 Score: 0.0000\n\nPrompt 5: Why do we see lightning before thunder?\nGenerated: Why do we see lightning before thunder?\n\nJake: (smiling) Because lightning is a warning signal. It's a sign that a storm is coming.\n\nMary: (smiling) That's interesting. So, what's the difference between lightning and thunder?\n\nJake: (smiling) Lightning is a flash of electricity, while thunder is the sound that follows.\n\nMary: (smiling) That's right. So, what's the best way to protect yourself from lightning?\n\nJake: (smiling) The best way to protect yourself from lightning is to be in a safe place, away\nReference: Light travels faster than sound, so we see lightning before we hear thunder.\nBLEU-4 Score: 0.0275\n\nPrompt 6: How do plants make food?\nGenerated: How do plants make food?\nReference: Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\nBLEU-4 Score: 0.0051\n\nPrompt 7: Why do we sleep?\nGenerated: Why do we sleep?\n\nSleep is a natural process that helps us recharge and regenerate. It is essential for our physical and mental health, and it plays a crucial role in our daily lives. Without sleep, we would be unable to function properly, and our bodies would be unable to repair and recover from the stresses and demands of daily life.\n\nIn this essay, we will explore the various benefits of sleep, including its impact on our physical and mental health, the role it plays in our daily lives, and the various sleep disorders that can arise. We will also discuss the various sleeping habits that are recommended by experts, such as the amount of sleep we need, the\nReference: We sleep to allow the body and brain to rest, recover, and process information from the day.\nBLEU-4 Score: 0.0088\n\nPrompt 8: What is photosynthesis?\nGenerated: What is photosynthesis?\nReference: Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\nBLEU-4 Score: 0.0007\n\nPrompt 9: How does the internet work?\nGenerated: How does the internet work?\nReference: The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\nBLEU-4 Score: 0.0041\n\nPrompt 10: Why do birds migrate?\nGenerated: Why do birds migrate?\n\nBirds migrate to find food and a new breeding ground. They travel long distances to reach their destination, which can be thousands of miles away. The migration process involves a combination of factors, including the availability of food, the weather, and the timing of the breeding season.\n\nThe timing of the breeding season is crucial for birds. The timing of the breeding season is determined by the availability of food and the length of the day. During the breeding season, birds need to find food and a suitable breeding site. If the breeding season is too early or too late, the birds may not be able to find enough food or the breeding\nReference: Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\nBLEU-4 Score: 0.0228\n\nAverage BLEU-4 Score across all prompts: 0.0145\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# DPO","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"946e923f7717c88464dc01b43cdcb664b74b23b6\"\nfrom trl import DPOConfig, DPOTrainer\n\nSft_model = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000', torch_dtype=torch.bfloat16).to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama-new-1000\")\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntrain_dataset = train_dataset.select(range(500))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:30.397300Z","iopub.execute_input":"2025-06-09T18:23:30.397497Z","iopub.status.idle":"2025-06-09T18:23:38.906260Z","shell.execute_reply.started":"2025-06-09T18:23:30.397482Z","shell.execute_reply":"2025-06-09T18:23:38.905628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/643 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab24a7957dca43dc951f70bae7f1b9cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005c5c2bf00e4e8eaeb8370513a50555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/2.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e53e11b9add482a9c6110f42c6703c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/62135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"627120ca751240ca90e90d03fdfcfa66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bedf81418e74c3e933285651dd66a16"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"Sft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:38.907273Z","iopub.execute_input":"2025-06-09T18:23:38.907584Z","iopub.status.idle":"2025-06-09T18:23:38.914801Z","shell.execute_reply.started":"2025-06-09T18:23:38.907558Z","shell.execute_reply":"2025-06-09T18:23:38.914033Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): lora.Linear(\n            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n            (lora_dropout): ModuleDict(\n              (default): Dropout(p=0.05, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (default): Linear(in_features=2048, out_features=2, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (default): Linear(in_features=2, out_features=2048, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"training_args = DPOConfig(\n    per_device_train_batch_size=1,  # Reduce batch size\n    remove_unused_columns=False,\n    max_length=150,  # Reduce sequence length if possible\n    max_prompt_length=256,\n    bf16 = True,  # To maintain effective batch size\n    dataloader_num_workers=0,\n)\n\n\ntrainer = DPOTrainer(model=Sft_model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:38.915627Z","iopub.execute_input":"2025-06-09T18:23:38.915927Z","iopub.status.idle":"2025-06-09T18:23:56.260304Z","shell.execute_reply.started":"2025-06-09T18:23:38.915909Z","shell.execute_reply":"2025-06-09T18:23:56.259172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting prompt in train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f43a87629d411bbc6bbd908e2d7bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61fb67800454b47abcaeb6d39e07167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a405d36aceb45deb960f5fe6ec377c5"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtahashamji45\u001b[0m (\u001b[33mtahashamji45-institute-of-business-administration\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250609_182348-h4fy52u3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface/runs/h4fy52u3' target=\"_blank\">trainer_output</a></strong> to <a href='https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface' target=\"_blank\">https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface/runs/h4fy52u3' target=\"_blank\">https://wandb.ai/tahashamji45-institute-of-business-administration/huggingface/runs/h4fy52u3</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/942046362.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3780\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3782\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3784\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"],"ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","output_type":"error"}],"execution_count":27}]}