{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:01:59.990494Z","iopub.execute_input":"2025-06-10T16:01:59.990774Z","iopub.status.idle":"2025-06-10T16:02:43.237025Z","shell.execute_reply.started":"2025-06-10T16:01:59.990754Z","shell.execute_reply":"2025-06-10T16:02:43.236489Z"}},"outputs":[{"name":"stderr","text":"2025-06-10 16:02:23.293839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749571343.756630      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749571343.871596      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:02:43.238055Z","iopub.execute_input":"2025-06-10T16:02:43.238575Z","iopub.status.idle":"2025-06-10T16:04:17.693458Z","shell.execute_reply.started":"2025-06-10T16:02:43.238556Z","shell.execute_reply":"2025-06-10T16:04:17.692721Z"}},"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nRequirement already satisfied: transformers>=4.50.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.3)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (0.21.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.18.1-py3-none-any.whl (366 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.18.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:17.694398Z","iopub.execute_input":"2025-06-10T16:04:17.694644Z","iopub.status.idle":"2025-06-10T16:04:17.892736Z","shell.execute_reply.started":"2025-06-10T16:04:17.694602Z","shell.execute_reply":"2025-06-10T16:04:17.892124Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:17.894590Z","iopub.execute_input":"2025-06-10T16:04:17.895254Z","iopub.status.idle":"2025-06-10T16:04:17.911970Z","shell.execute_reply.started":"2025-06-10T16:04:17.895209Z","shell.execute_reply":"2025-06-10T16:04:17.911452Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:17.912621Z","iopub.execute_input":"2025-06-10T16:04:17.912819Z","iopub.status.idle":"2025-06-10T16:04:17.931062Z","shell.execute_reply.started":"2025-06-10T16:04:17.912795Z","shell.execute_reply":"2025-06-10T16:04:17.930554Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:17.931687Z","iopub.execute_input":"2025-06-10T16:04:17.931924Z","iopub.status.idle":"2025-06-10T16:04:18.975784Z","shell.execute_reply.started":"2025-06-10T16:04:17.931907Z","shell.execute_reply":"2025-06-10T16:04:18.975275Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ed3612283624adaaf2da7b83d78996b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01874b6f750647cdb6505a37df6aa91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3403bdb07244dc899ea7ac99c7e8c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc0664a1393048da96f18ca69237816c"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Load dataset and tokenizer\ndataset = load_dataset(\"allenai/soda\")\n\n\n\ndef tokenize_function(examples):\n    # Concatenate dialog turns into a single string for language modeling\n    texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n    return tokenizer(texts, truncation=True, max_length=512)\n\n# Tokenize datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'])\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:04:18.976512Z","iopub.execute_input":"2025-06-10T16:04:18.976713Z","iopub.status.idle":"2025-06-10T16:10:51.531626Z","shell.execute_reply.started":"2025-06-10T16:04:18.976697Z","shell.execute_reply":"2025-06-10T16:10:51.530807Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ae98459b87447e9272add2bd0d2fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/689M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce56f74cdf044fb89cf66175f449427e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.parquet:   0%|          | 0.00/82.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e72d226a1764f3faa594a98556ea1d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a2dc153fec43d290953624c1bc9854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55fe075cc813406ab09ec616b2cf847b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88188fce5a9940c1a45c4f10250920ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba600b52fdcd44c39bd366cfaa0f9b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c5e3b9abc542a4a7ee0a3eea365b49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2e81ad16df49a4b3ce950f9e587062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68c7613192748b29fce3ff09e682cc4"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(dataset)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:10:51.532540Z","iopub.execute_input":"2025-06-10T16:10:51.533010Z","iopub.status.idle":"2025-06-10T16:10:51.538609Z","shell.execute_reply.started":"2025-06-10T16:10:51.532978Z","shell.execute_reply":"2025-06-10T16:10:51.537995Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n})\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 148968\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=1,\n    lora_alpha=32,\n    lora_dropout=0.09,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:10:51.539403Z","iopub.execute_input":"2025-06-10T16:10:51.539672Z","iopub.status.idle":"2025-06-10T16:10:51.636600Z","shell.execute_reply.started":"2025-06-10T16:10:51.539630Z","shell.execute_reply":"2025-06-10T16:10:51.636028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:10:51.638720Z","iopub.execute_input":"2025-06-10T16:10:51.638934Z","iopub.status.idle":"2025-06-10T16:11:15.005122Z","shell.execute_reply.started":"2025-06-10T16:10:51.638919Z","shell.execute_reply":"2025-06-10T16:11:15.004423Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063a142556de4e6f9295b718f4d7e758"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b124cf0fa9104c0cb749a005d72f9fbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0914b4cce3435c995841d5e863b617"}},"metadata":{}},{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.43 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:15.006103Z","iopub.execute_input":"2025-06-10T16:11:15.006399Z","iopub.status.idle":"2025-06-10T16:11:15.056056Z","shell.execute_reply.started":"2025-06-10T16:11:15.006371Z","shell.execute_reply":"2025-06-10T16:11:15.055387Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.44 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:15.056891Z","iopub.execute_input":"2025-06-10T16:11:15.057131Z","iopub.status.idle":"2025-06-10T16:11:15.217004Z","shell.execute_reply.started":"2025-06-10T16:11:15.057115Z","shell.execute_reply":"2025-06-10T16:11:15.216499Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:15.217579Z","iopub.execute_input":"2025-06-10T16:11:15.217740Z","iopub.status.idle":"2025-06-10T16:11:15.222498Z","shell.execute_reply.started":"2025-06-10T16:11:15.217727Z","shell.execute_reply":"2025-06-10T16:11:15.221932Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:15.223152Z","iopub.execute_input":"2025-06-10T16:11:15.223420Z","iopub.status.idle":"2025-06-10T16:13:22.551937Z","shell.execute_reply.started":"2025-06-10T16:11:15.223399Z","shell.execute_reply":"2025-06-10T16:13:22.551395Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 02:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.803200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.775800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=0.7808976593017578, metrics={'train_runtime': 126.8848, 'train_samples_per_second': 3.941, 'train_steps_per_second': 1.97, 'total_flos': 636044645670912.0, 'train_loss': 0.7808976593017578, 'epoch': 1.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:13:22.552722Z","iopub.execute_input":"2025-06-10T16:13:22.553427Z","iopub.status.idle":"2025-06-10T16:13:22.721290Z","shell.execute_reply.started":"2025-06-10T16:13:22.553408Z","shell.execute_reply":"2025-06-10T16:13:22.720758Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:13:22.721966Z","iopub.execute_input":"2025-06-10T16:13:22.722194Z","iopub.status.idle":"2025-06-10T16:13:22.726736Z","shell.execute_reply.started":"2025-06-10T16:13:22.722177Z","shell.execute_reply":"2025-06-10T16:13:22.725979Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.07 GB, Cached: 6.29 GB\n[CPU] Memory Usage: 3.00 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nsmoothie = SmoothingFunction().method4\n# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_texts = [\n    \"So light is made up of colors?\",\n    \"Why is the sky blue?\",\n    \"How does gravity work?\",\n    \"What causes rainbows?\",\n    \"Why do we see lightning before thunder?\",\n    \"How do plants make food?\",\n    \"Why do we sleep?\",\n    \"What is photosynthesis?\",\n    \"How does the internet work?\",\n    \"Why do birds migrate?\"\n]\n\nreference_answers = [\n    \"Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\",\n    \"The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\",\n    \"Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\",\n    \"Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\",\n    \"Light travels faster than sound, so we see lightning before we hear thunder.\",\n    \"Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\",\n    \"We sleep to allow the body and brain to rest, recover, and process information from the day.\",\n    \"Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\",\n    \"The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\",\n    \"Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\"\n]\n\nbleu_scores = []\n\nfor i in range(10):\n    input_text = input_texts[i]\n    reference = reference_answers[i]\n\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    output = model_for_generation.generate(\n        input_ids,\n        max_length=70,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    candidate = generated_response.split()\n    reference_tokens = [reference.split()]\n\n    score = sentence_bleu(reference_tokens, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n    bleu_scores.append(score)\n\n    print(f\"\\nPrompt {i+1}: {input_text}\")\n    print(f\"Generated: {generated_response}\")\n    print(f\"Reference: {reference}\")\n    print(f\"BLEU-4 Score: {score:.4f}\")\n\nprint(\"\\nAverage BLEU-4 Score across all prompts: %.4f\" % (sum(bleu_scores) / len(bleu_scores)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:13:22.727719Z","iopub.execute_input":"2025-06-10T16:13:22.727961Z","iopub.status.idle":"2025-06-10T16:13:35.479792Z","shell.execute_reply.started":"2025-06-10T16:13:22.727942Z","shell.execute_reply":"2025-06-10T16:13:35.479140Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"\nPrompt 1: So light is made up of colors?\nGenerated: So light is made up of colors?\nReference: Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\nBLEU-4 Score: 0.0721\n\nPrompt 2: Why is the sky blue?\nGenerated: Why is the sky blue?\n\n1. The sky is blue because it reflects the sun's rays.\n\n2. The sun's rays are blue because they have a high frequency of light.\n\n3. The blue color of the sky is caused by the reflection of the sun's rays.\nReference: The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\nBLEU-4 Score: 0.0175\n\nPrompt 3: How does gravity work?\nGenerated: How does gravity work?\nReference: Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\nBLEU-4 Score: 0.0000\n\nPrompt 4: What causes rainbows?\nGenerated: What causes rainbows?\nReference: Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\nBLEU-4 Score: 0.0000\n\nPrompt 5: Why do we see lightning before thunder?\nGenerated: Why do we see lightning before thunder?\n\nLightning is caused by the release of energy from the electrostatic forces between two charged objects. When a cloud is struck by lightning, the positive ions in the cloud are attracted to the negative ions in the ground, creating a potential difference that can trigger a spark.\nReference: Light travels faster than sound, so we see lightning before we hear thunder.\nBLEU-4 Score: 0.0422\n\nPrompt 6: How do plants make food?\nGenerated: How do plants make food?\nReference: Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\nBLEU-4 Score: 0.0051\n\nPrompt 7: Why do we sleep?\nGenerated: Why do we sleep?\n\nSleep is a natural process that helps us recharge and refresh our bodies. It's a time when our minds and bodies can rest and recover from the day's activities.\n\n2. What is the purpose of dreaming?\n\nDreaming is a natural part of our sleep cycle\nReference: We sleep to allow the body and brain to rest, recover, and process information from the day.\nBLEU-4 Score: 0.0180\n\nPrompt 8: What is photosynthesis?\nGenerated: What is photosynthesis?\nReference: Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\nBLEU-4 Score: 0.0007\n\nPrompt 9: How does the internet work?\nGenerated: How does the internet work?\nReference: The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\nBLEU-4 Score: 0.0041\n\nPrompt 10: Why do birds migrate?\nGenerated: Why do birds migrate?\n\nBirds migrate to find food, water, and a suitable breeding and nesting site. They also migrate to avoid extreme weather conditions, such as heat waves or cold snaps. Some birds migrate long distances, while others migrate only a few hundred miles.\n\n2.\nReference: Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\nBLEU-4 Score: 0.0572\n\nAverage BLEU-4 Score across all prompts: 0.0217\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# DPO","metadata":{}},{"cell_type":"code","source":"# train_dpo.py\nimport os\nos.environ[\"WANDB_MODE\"] = \"offline\"\nos.environ[\"WANDB_API_KEY\"] = \"946e923f7717c88464dc01b43cdcb664b74b23b6\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:15:26.643125Z","iopub.execute_input":"2025-06-10T16:15:26.643476Z","iopub.status.idle":"2025-06-10T16:15:26.647936Z","shell.execute_reply.started":"2025-06-10T16:15:26.643452Z","shell.execute_reply":"2025-06-10T16:15:26.647017Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model, AutoPeftModelForCausalLM\n\nmodel_name = \"TinyLlama-new-1000\"  # or any other base model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Add padding token if missing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:15:29.409220Z","iopub.execute_input":"2025-06-10T16:15:29.409905Z","iopub.status.idle":"2025-06-10T16:15:30.725770Z","shell.execute_reply.started":"2025-06-10T16:15:29.409883Z","shell.execute_reply":"2025-06-10T16:15:30.724913Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=4,  # rank\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\",\"v_proj\"]  # adjust based on model\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:15:34.815708Z","iopub.execute_input":"2025-06-10T16:15:34.816201Z","iopub.status.idle":"2025-06-10T16:15:34.844338Z","shell.execute_reply.started":"2025-06-10T16:15:34.816174Z","shell.execute_reply":"2025-06-10T16:15:34.843774Z"}},"outputs":[{"name":"stdout","text":"trainable params: 90,112 || all params: 1,100,138,496 || trainable%: 0.0082\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nfrom datasets import load_dataset\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntrain_dataset = train_dataset.select(range(500))\n\n# Format the data\ndata = []\nfor item in train_dataset:\n    try:\n        prompt = next((x[\"content\"] for x in item[\"chosen\"] if x[\"role\"] == \"user\"), \"\")\n        chosen = next((x[\"content\"] for x in item[\"chosen\"] if x[\"role\"] == \"assistant\"), \"\")\n        rejected = next((x[\"content\"] for x in item[\"rejected\"] if x[\"role\"] == \"assistant\"), \"\")\n        \n        data.append({\n            \"prompt\": prompt,\n            \"chosen\": chosen,\n            \"rejected\": rejected\n        })\n    except Exception:\n        continue  # Skip entries that don't have the expected format\n\n# Create HuggingFace dataset\nformatted_dataset = Dataset.from_list(data)\n\n\n\n# Preview\nprint(formatted_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:15:40.015463Z","iopub.execute_input":"2025-06-10T16:15:40.015987Z","iopub.status.idle":"2025-06-10T16:15:44.283325Z","shell.execute_reply.started":"2025-06-10T16:15:40.015965Z","shell.execute_reply":"2025-06-10T16:15:44.282736Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/643 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e230c388db6f45a3af74393acbbd60fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e8d62a9fed4da9a1866615abfa4184"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/2.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af608e9c3be409da9bbfe029de526f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/62135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1edd8bdca1e24b2189458acd79d09e0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13838a20e9404c118f74704f90b99cef"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 500\n})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from trl import DPOTrainer, DPOConfig\n\n# Training arguments\ntraining_args = DPOConfig(\n    output_dir=\"./dpo-model\",\n    num_train_epochs=2,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,\n    logging_steps=10,\n    save_steps=500,\n    eval_steps=500,\n    warmup_steps=20,\n    beta=0.1,  # DPO beta parameter\n    remove_unused_columns=False,\n)\n\n# Initialize DPO trainer\ndpo_trainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=formatted_dataset,\n   processing_class=tokenizer,\n    peft_config=peft_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:48:57.996390Z","iopub.execute_input":"2025-06-10T16:48:57.996999Z","iopub.status.idle":"2025-06-10T16:48:59.356772Z","shell.execute_reply.started":"2025-06-10T16:48:57.996977Z","shell.execute_reply":"2025-06-10T16:48:59.356222Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting prompt in train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1571491e85184256b51faa6422814fd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a04c40efeeb948dc92efbb832e522225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73119d887d74e848ab9db9ce81862a1"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Start training\ndpo_trainer.train()\n\n# Save the model\ndpo_trainer.save_model(\"./final-dpo-model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nsmoothie = SmoothingFunction().method4\n# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('final-dpo-model').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_texts = [\n    \"So light is made up of colors?\",\n    \"Why is the sky blue?\",\n    \"How does gravity work?\",\n    \"What causes rainbows?\",\n    \"Why do we see lightning before thunder?\",\n    \"How do plants make food?\",\n    \"Why do we sleep?\",\n    \"What is photosynthesis?\",\n    \"How does the internet work?\",\n    \"Why do birds migrate?\"\n]\n\nreference_answers = [\n    \"Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\",\n    \"The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\",\n    \"Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\",\n    \"Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\",\n    \"Light travels faster than sound, so we see lightning before we hear thunder.\",\n    \"Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\",\n    \"We sleep to allow the body and brain to rest, recover, and process information from the day.\",\n    \"Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\",\n    \"The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\",\n    \"Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\"\n]\n\nbleu_scores = []\n\nfor i in range(10):\n    input_text = input_texts[i]\n    reference = reference_answers[i]\n\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\n    output = model_for_generation.generate(\n        input_ids,\n        max_length=70,\n        num_return_sequences=1,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    candidate = generated_response.split()\n    reference_tokens = [reference.split()]\n\n    score = sentence_bleu(reference_tokens, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n    bleu_scores.append(score)\n\n    print(f\"\\nPrompt {i+1}: {input_text}\")\n    print(f\"Generated: {generated_response}\")\n    print(f\"Reference: {reference}\")\n    print(f\"BLEU-4 Score: {score:.4f}\")\n\nprint(\"\\nAverage BLEU-4 Score across all prompts: %.4f\" % (sum(bleu_scores) / len(bleu_scores)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:46:48.782823Z","iopub.execute_input":"2025-06-10T16:46:48.783588Z","iopub.status.idle":"2025-06-10T16:46:59.856267Z","shell.execute_reply.started":"2025-06-10T16:46:48.783563Z","shell.execute_reply":"2025-06-10T16:46:59.855543Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nPrompt 1: So light is made up of colors?\nGenerated: So light is made up of colors?\nReference: Yes, light is made up of different colors. When they combine, they appear white. This can be separated with prisms, for example.\nBLEU-4 Score: 0.0721\n\nPrompt 2: Why is the sky blue?\nGenerated: Why is the sky blue?\n\n1. The sun is the primary source of light in the sky.\n\n2. The Earth's atmosphere absorbs and reflects some of the sun's light.\n\n3. The Earth's rotation around the sun causes the sun's rays to travel in a circular path\nReference: The sky appears blue due to a phenomenon called Rayleigh scattering. Shorter blue wavelengths scatter more than red ones.\nBLEU-4 Score: 0.0092\n\nPrompt 3: How does gravity work?\nGenerated: How does gravity work?\nReference: Gravity is a force that attracts objects with mass toward each other. Earth pulls objects towards its center due to gravity.\nBLEU-4 Score: 0.0000\n\nPrompt 4: What causes rainbows?\nGenerated: What causes rainbows?\nReference: Rainbows occur when sunlight is both refracted and reflected inside raindrops, separating the light into different colors.\nBLEU-4 Score: 0.0000\n\nPrompt 5: Why do we see lightning before thunder?\nGenerated: Why do we see lightning before thunder?\n\nJake: (smiling) Because lightning is a warning signal. It's a sign that a storm is coming.\n\nMary: (smiling) That's interesting. So, what's the difference between lightning and thunder?\n\nJ\nReference: Light travels faster than sound, so we see lightning before we hear thunder.\nBLEU-4 Score: 0.0642\n\nPrompt 6: How do plants make food?\nGenerated: How do plants make food?\nReference: Plants make food through photosynthesis, using sunlight, water, and carbon dioxide to produce energy and oxygen.\nBLEU-4 Score: 0.0051\n\nPrompt 7: Why do we sleep?\nGenerated: Why do we sleep?\n\nSleep is a natural process that helps us recharge and regenerate. It is essential for our physical and mental health, and it plays a crucial role in our daily lives. Without sleep, we would be unable to function properly, and our bodies would be unable to repair and recover from the str\nReference: We sleep to allow the body and brain to rest, recover, and process information from the day.\nBLEU-4 Score: 0.0159\n\nPrompt 8: What is photosynthesis?\nGenerated: What is photosynthesis?\nReference: Photosynthesis is the process plants use to convert sunlight into energy using chlorophyll, water, and carbon dioxide.\nBLEU-4 Score: 0.0007\n\nPrompt 9: How does the internet work?\nGenerated: How does the internet work?\nReference: The internet is a global network of computers that communicate via protocols like TCP/IP to exchange data.\nBLEU-4 Score: 0.0041\n\nPrompt 10: Why do birds migrate?\nGenerated: Why do birds migrate?\n\nBirds migrate to find food and a new breeding ground. They travel long distances to reach their destination, which can be thousands of miles away. The migration process involves a combination of factors, including the availability of food, the weather, and the timing of the breeding season.\n\nReference: Birds migrate to find better food sources, breeding grounds, or climates that are more suitable during different seasons.\nBLEU-4 Score: 0.0495\n\nAverage BLEU-4 Score across all prompts: 0.0221\n","output_type":"stream"}],"execution_count":25}]}