{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:46.912884Z","iopub.execute_input":"2025-06-09T12:37:46.913572Z","iopub.status.idle":"2025-06-09T12:37:46.917342Z","shell.execute_reply.started":"2025-06-09T12:37:46.913548Z","shell.execute_reply":"2025-06-09T12:37:46.916791Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:49:02.014699Z","iopub.execute_input":"2025-06-09T12:49:02.014995Z","iopub.status.idle":"2025-06-09T12:50:37.545778Z","shell.execute_reply.started":"2025-06-09T12:49:02.014973Z","shell.execute_reply":"2025-06-09T12:50:37.545027Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.5.2)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nRequirement already satisfied: transformers>=4.50.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.51.3)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.50.0->trl) (0.21.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate>=0.34.0->trl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\nDownloading trl-0.18.1-py3-none-any.whl (366 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, trl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.18.1\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:46.922337Z","iopub.execute_input":"2025-06-09T12:37:46.922575Z","iopub.status.idle":"2025-06-09T12:37:46.945476Z","shell.execute_reply.started":"2025-06-09T12:37:46.922561Z","shell.execute_reply":"2025-06-09T12:37:46.944970Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:46.946646Z","iopub.execute_input":"2025-06-09T12:37:46.946867Z","iopub.status.idle":"2025-06-09T12:37:46.963179Z","shell.execute_reply.started":"2025-06-09T12:37:46.946851Z","shell.execute_reply":"2025-06-09T12:37:46.962719Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:46.963796Z","iopub.execute_input":"2025-06-09T12:37:46.964015Z","iopub.status.idle":"2025-06-09T12:37:46.982018Z","shell.execute_reply.started":"2025-06-09T12:37:46.963999Z","shell.execute_reply":"2025-06-09T12:37:46.981398Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:46.983233Z","iopub.execute_input":"2025-06-09T12:37:46.983416Z","iopub.status.idle":"2025-06-09T12:37:47.182040Z","shell.execute_reply.started":"2025-06-09T12:37:46.983402Z","shell.execute_reply":"2025-06-09T12:37:47.181474Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Load dataset and tokenizer\ndataset = load_dataset(\"allenai/soda\")\n\n\n\ndef tokenize_function(examples):\n    # Concatenate dialog turns into a single string for language modeling\n    texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n    return tokenizer(texts, truncation=True, max_length=512)\n\n# Tokenize datasets\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'])\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:37:47.182648Z","iopub.execute_input":"2025-06-09T12:37:47.182828Z","iopub.status.idle":"2025-06-09T12:38:20.541351Z","shell.execute_reply.started":"2025-06-09T12:37:47.182814Z","shell.execute_reply":"2025-06-09T12:38:20.540587Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2173b7f58dc54ebba94740b173f9b75d"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(dataset)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:20.542354Z","iopub.execute_input":"2025-06-09T12:38:20.542629Z","iopub.status.idle":"2025-06-09T12:38:20.547887Z","shell.execute_reply.started":"2025-06-09T12:38:20.542582Z","shell.execute_reply":"2025-06-09T12:38:20.547178Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n})\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 148968\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:20.548548Z","iopub.execute_input":"2025-06-09T12:38:20.548823Z","iopub.status.idle":"2025-06-09T12:38:20.999266Z","shell.execute_reply.started":"2025-06-09T12:38:20.548801Z","shell.execute_reply":"2025-06-09T12:38:20.998714Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:21.001184Z","iopub.execute_input":"2025-06-09T12:38:21.001390Z","iopub.status.idle":"2025-06-09T12:38:32.469612Z","shell.execute_reply.started":"2025-06-09T12:38:21.001373Z","shell.execute_reply":"2025-06-09T12:38:32.468887Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"270b3933c00a435eb2d7b6046d9ab0d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a458d8b56f43ea836b9c914d8f8933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf03fd4de0ec46bc9681579d9f07d1c3"}},"metadata":{}},{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.39 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=1000,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:32.470444Z","iopub.execute_input":"2025-06-09T12:38:32.470762Z","iopub.status.idle":"2025-06-09T12:38:32.512309Z","shell.execute_reply.started":"2025-06-09T12:38:32.470740Z","shell.execute_reply":"2025-06-09T12:38:32.511516Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.40 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:32.513171Z","iopub.execute_input":"2025-06-09T12:38:32.513890Z","iopub.status.idle":"2025-06-09T12:38:33.469949Z","shell.execute_reply.started":"2025-06-09T12:38:32.513872Z","shell.execute_reply":"2025-06-09T12:38:33.469355Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\nmodel_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n             torch_dtype=torch.bfloat16).to(device)\n\ninput_text_before = \"Hello, how are you?\"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:33.470575Z","iopub.execute_input":"2025-06-09T12:38:33.470848Z","iopub.status.idle":"2025-06-09T12:38:38.877342Z","shell.execute_reply.started":"2025-06-09T12:38:33.470830Z","shell.execute_reply":"2025-06-09T12:38:38.876733Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response (Before): Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again,\n---\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\n#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\ninput_text_before = \"The USA celebrate independence day on \"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:38.878090Z","iopub.execute_input":"2025-06-09T12:38:38.878333Z","iopub.status.idle":"2025-06-09T12:38:40.134613Z","shell.execute_reply.started":"2025-06-09T12:38:38.878305Z","shell.execute_reply":"2025-06-09T12:38:40.133974Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: The USA celebrate independence day on \nGenerated Response (Before): The USA celebrate independence day on 4th July.\n\n2. The USA celebrate independence day on 4th July.\n\n3. The USA celebrate independence day on 4th July.\n\n4. The\n---\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:40.135323Z","iopub.execute_input":"2025-06-09T12:38:40.135608Z","iopub.status.idle":"2025-06-09T12:38:44.689623Z","shell.execute_reply.started":"2025-06-09T12:38:40.135568Z","shell.execute_reply":"2025-06-09T12:38:44.688811Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:38:44.690288Z","iopub.execute_input":"2025-06-09T12:38:44.690498Z","iopub.status.idle":"2025-06-09T12:48:00.425867Z","shell.execute_reply.started":"2025-06-09T12:38:44.690482Z","shell.execute_reply":"2025-06-09T12:48:00.425299Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 09:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.75694677734375, metrics={'train_runtime': 554.373, 'train_samples_per_second': 3.608, 'train_steps_per_second': 0.902, 'total_flos': 3150837652340736.0, 'train_loss': 0.75694677734375, 'epoch': 1.0})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:00.426555Z","iopub.execute_input":"2025-06-09T12:48:00.426834Z","iopub.status.idle":"2025-06-09T12:48:00.560443Z","shell.execute_reply.started":"2025-06-09T12:48:00.426812Z","shell.execute_reply":"2025-06-09T12:48:00.559718Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:00.561249Z","iopub.execute_input":"2025-06-09T12:48:00.561439Z","iopub.status.idle":"2025-06-09T12:48:00.566004Z","shell.execute_reply.started":"2025-06-09T12:48:00.561426Z","shell.execute_reply":"2025-06-09T12:48:00.565424Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 4.12 GB, Cached: 9.63 GB\n[CPU] Memory Usage: 3.10 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_text = \"Hello, how are you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")\n\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:00.566755Z","iopub.execute_input":"2025-06-09T12:48:00.566965Z","iopub.status.idle":"2025-06-09T12:48:07.961664Z","shell.execute_reply.started":"2025-06-09T12:48:00.566940Z","shell.execute_reply":"2025-06-09T12:48:07.960982Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nGenerating response:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response: Hello, how are you? I'm good, thanks. How about you? I'm doing well. I'm glad to hear that. You sound good too. That's good to hear. I'm glad to hear that too. You're doing great. That's great to hear. I'm glad to hear that too. You're doing great. That's great to hear. I'm glad to hear that too. You're doing great. That's great to hear. I'm glad to hear that too. You're doing great. That's great to hear. I'm glad to hear that too. You're doing great. That's\n[GPU] Memory Allocated: 8.22 GB, Cached: 10.63 GB\n[CPU] Memory Usage: 4.23 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"input_text = \"The USA celebrate independence day on \"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:07.962379Z","iopub.execute_input":"2025-06-09T12:48:07.962635Z","iopub.status.idle":"2025-06-09T12:48:11.778203Z","shell.execute_reply.started":"2025-06-09T12:48:07.962607Z","shell.execute_reply":"2025-06-09T12:48:11.777455Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: The USA celebrate independence day on \nGenerated Response: The USA celebrate independence day on 4th July.\n\n2. India Independence Day is celebrated on 15th August.\n\n3. Canada celebrates its 150th birthday on July 1st.\n\n4. Australia celebrates its 238th birthday on 26th January.\n\n5. New Zealand celebrates its 238th birthday on 26th January.\n\n6. South Africa celebrates its 238th birthday on 26th January.\n\n7. United States celebrates its 241st birthday on 4th July.\n\n8. United Kingdom celebrates its\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"input_text = \"So light is made up of colors?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:11.780670Z","iopub.execute_input":"2025-06-09T12:48:11.780917Z","iopub.status.idle":"2025-06-09T12:48:15.718468Z","shell.execute_reply.started":"2025-06-09T12:48:11.780900Z","shell.execute_reply":"2025-06-09T12:48:15.717872Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: So light is made up of colors?\nGenerated Response: So light is made up of colors?\nYes, light is made up of colors. Light is made up of electromagnetic waves, which are made up of tiny particles called photons. Each photon has a specific wavelength, which determines its color. The colors of light are determined by the wavelength of the photons that make up the light. The colors of light are determined by the frequency of the photons. The higher the frequency, the shorter the wavelength, and the more blue or violet the color. The lower the frequency, the longer the wavelength, and the more green or yellow the color. The colors of light are determined by the angle at which the photons are incident\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"small_eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:15.719158Z","iopub.execute_input":"2025-06-09T12:48:15.719412Z","iopub.status.idle":"2025-06-09T12:48:15.724240Z","shell.execute_reply.started":"2025-06-09T12:48:15.719392Z","shell.execute_reply":"2025-06-09T12:48:15.723623Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\n\n# Tokenize the generated response\ncandidate = generated_response.split()\n\n# Define reference responses (tokenized)\nreferences = [\n    \"Yes, that's right. Light is made up of different colors, and when those colors mix together, we see white light. But light can also bend and bounce off surfaces.\",\n    \"How does it do that?\",\n    \"When light hits a surface, it can either reflect off of the surface or refract through the surface. Reflection is when the light bounces off of the surface, and refraction is when the light bends as it passes through the surface.\",\n    \"So when light reflects off of a mirror, it's because the light is bouncing off of the surface of the mirror?\",\n    \"That's right. Mirrors are very good at reflecting light because they have a smooth, flat surface.\",\n    \"And when light refracts through something like a prism, it's because the light is bending as it passes through the surface of the prism?\",\n    \"That's right. When light passes through a prism, it bends because the Prism bends the light.\"\n]\n\n# Tokenize each reference\nreference_tokens = [ref.split() for ref in references]\n\n# Calculate BLEU scores\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmoothie = SmoothingFunction().method4  # Helps with zero scores for higher n-grams\n\nprint('Individual 1-gram: %f' % sentence_bleu(reference_tokens, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie))\nprint('Individual 2-gram: %f' % sentence_bleu(reference_tokens, candidate, weights=(0, 1, 0, 0), smoothing_function=smoothie))\nprint('Individual 3-gram: %f' % sentence_bleu(reference_tokens, candidate, weights=(0, 0, 1, 0), smoothing_function=smoothie))\nprint('Individual 4-gram: %f' % sentence_bleu(reference_tokens, candidate, weights=(0, 0, 0, 1), smoothing_function=smoothie))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T12:48:15.724896Z","iopub.execute_input":"2025-06-09T12:48:15.725131Z","iopub.status.idle":"2025-06-09T12:48:16.965932Z","shell.execute_reply.started":"2025-06-09T12:48:15.725108Z","shell.execute_reply":"2025-06-09T12:48:16.965311Z"}},"outputs":[{"name":"stdout","text":"Individual 1-gram: 0.232759\nIndividual 2-gram: 0.069565\nIndividual 3-gram: 0.026316\nIndividual 4-gram: 0.017699\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# DPO","metadata":{}},{"cell_type":"code","source":"\nfrom trl import DPOConfig, DPOTrainer\ntorch.cuda.empty_cache()\nSft_model = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\ntrain_dataset = train_dataset.select(range(2000))\n# print(train_dataset)\n# # DPO training config\n# training_args = DPOConfig(\n#     output_dir=\"TinyLlama-0.5B-DPO\",\n#     logging_steps=100,\n#     per_device_train_batch_size=1,\n#     num_train_epochs=1,\n#     remove_unused_columns=False,\n#     bf16=True,  # Enable bfloat16\n#     fp16=False,  # Disable fp16 to avoid conflicts\n# )\n# trainer = DPOTrainer(model=Sft_model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T13:09:34.257182Z","iopub.execute_input":"2025-06-09T13:09:34.257696Z","execution_failed":"2025-06-09T13:11:30.481Z"}},"outputs":[],"execution_count":null}]}