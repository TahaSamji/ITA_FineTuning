{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"829e0f85","cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport torch\nimport psutil\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:08:46.370276Z","iopub.execute_input":"2025-06-08T19:08:46.370490Z","iopub.status.idle":"2025-06-08T19:09:17.300281Z","shell.execute_reply.started":"2025-06-08T19:08:46.370473Z","shell.execute_reply":"2025-06-08T19:09:17.299658Z"}},"outputs":[{"name":"stderr","text":"2025-06-08 19:09:03.035621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749409743.256974      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749409743.319717      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"4ef75cd9","cell_type":"code","source":"torch.cuda.set_per_process_memory_fraction(1.0, 0)  # Use maximum available memory\ntorch.cuda.memory_max_split_size_mb = 64  # Set the max split size to avoid fragmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:09:17.300928Z","iopub.execute_input":"2025-06-08T19:09:17.301415Z","iopub.status.idle":"2025-06-08T19:09:17.475412Z","shell.execute_reply.started":"2025-06-08T19:09:17.301396Z","shell.execute_reply":"2025-06-08T19:09:17.474596Z"}},"outputs":[],"execution_count":2},{"id":"ad49ad5f","cell_type":"code","source":"def print_memory_footprint():\n    # GPU memory usage\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n        gpu_memory_cached = torch.cuda.memory_reserved() / (1024 ** 3)  # Cached memory\n        print(f\"[GPU] Memory Allocated: {gpu_memory:.2f} GB, Cached: {gpu_memory_cached:.2f} GB\")\n    else:\n        print(\"[GPU] No GPU detected.\")\n\n    # CPU memory usage\n    memory = psutil.virtual_memory()\n    used_memory_gb = memory.used / (1024 ** 3)  # Convert to GB\n    total_memory_gb = memory.total / (1024 ** 3)\n    print(f\"[CPU] Memory Usage: {used_memory_gb:.2f} GB / {total_memory_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:09:17.477562Z","iopub.execute_input":"2025-06-08T19:09:17.478025Z","iopub.status.idle":"2025-06-08T19:09:17.550404Z","shell.execute_reply.started":"2025-06-08T19:09:17.478004Z","shell.execute_reply":"2025-06-08T19:09:17.549716Z"}},"outputs":[],"execution_count":3},{"id":"f5b6efd2","cell_type":"code","source":"'''\nprint(\"First example of blended_skill_talk training set:\")\nprint(dataset['train'][0])\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:09:17.551062Z","iopub.execute_input":"2025-06-08T19:09:17.551301Z","iopub.status.idle":"2025-06-08T19:09:17.568455Z","shell.execute_reply.started":"2025-06-08T19:09:17.551284Z","shell.execute_reply":"2025-06-08T19:09:17.567866Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'\\nprint(\"First example of blended_skill_talk training set:\")\\nprint(dataset[\\'train\\'][0])\\n'"},"metadata":{}}],"execution_count":4},{"id":"bcee66dc","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token, so we use eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:09:17.569180Z","iopub.execute_input":"2025-06-08T19:09:17.569463Z","iopub.status.idle":"2025-06-08T19:09:18.728831Z","shell.execute_reply.started":"2025-06-08T19:09:17.569447Z","shell.execute_reply":"2025-06-08T19:09:18.728317Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb9beaf877e4b51a1e51957b9a0435f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8167c7249f740979fb6d3a655c967b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dedd49bd714b4362a8a88b6f6f675b93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73fed56b1d9945ecb0f9dfcd4c633410"}},"metadata":{}}],"execution_count":5},{"id":"a8d7cdff-5596-4b28-a2dc-d5e0e6e45d95","cell_type":"code","source":"# Load dataset and tokenizer\ndataset2 = load_dataset(\"allenai/soda\")\n\n\n\ndef tokenize_function(examples):\n    # Concatenate dialog turns into a single string for language modeling\n    texts = [\" \".join(dialog) for dialog in examples[\"dialogue\"]]\n    return tokenizer(texts, truncation=True, max_length=512)\n\n# Tokenize datasets\ntokenized_datasets = dataset2.map(tokenize_function, batched=True, remove_columns=['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'])\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:09:18.729848Z","iopub.execute_input":"2025-06-08T19:09:18.730949Z","iopub.status.idle":"2025-06-08T19:14:58.284713Z","shell.execute_reply.started":"2025-06-08T19:09:18.730921Z","shell.execute_reply":"2025-06-08T19:14:58.283960Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eab112c58ed4b3087929fc34bf75948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/689M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4d927bad3f419e8255fdad1a389298"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.parquet:   0%|          | 0.00/82.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20387551b82b4dcb968e9cd5e15fd98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e323cbf114f40b9b490045af461a7a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545c8859b65e4b12ab629550e7a01d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c6ab95952648e98d1e09f587ba40bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c99faaa01640b4975b82f3717084c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1191582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7757674463504fa9b832343b24cb567b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/146346 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"126bd97c1db14efba4ad25db40813dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/148968 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a9d5500dfa435e9b5468c2e99ae69b"}},"metadata":{}}],"execution_count":6},{"id":"be943d01-c616-49f3-999c-7f90c18bd511","cell_type":"code","source":"print(dataset2)\ntokenized_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:14:58.285656Z","iopub.execute_input":"2025-06-08T19:14:58.286245Z","iopub.status.idle":"2025-06-08T19:14:58.291591Z","shell.execute_reply.started":"2025-06-08T19:14:58.286214Z","shell.execute_reply":"2025-06-08T19:14:58.290815Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['head', 'relation', 'tail', 'literal', 'narrative', 'dialogue', 'speakers', 'PersonX', 'PersonY', 'PersonZ', 'original_index', 'split', 'head_answer', 'pmi_head_answer', 'relation_tail_answer', 'pmi_relation_tail_answer'],\n        num_rows: 148968\n    })\n})\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 1191582\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 146346\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 148968\n    })\n})"},"metadata":{}}],"execution_count":7},{"id":"f24655c4","cell_type":"code","source":"# LoRA configuration for causal language modeling\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=2,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\"],\n)\n\n# %%","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:14:58.292341Z","iopub.execute_input":"2025-06-08T19:14:58.292643Z","iopub.status.idle":"2025-06-08T19:14:58.784231Z","shell.execute_reply.started":"2025-06-08T19:14:58.292617Z","shell.execute_reply":"2025-06-08T19:14:58.783524Z"}},"outputs":[],"execution_count":8},{"id":"f418c726","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load pre-trained GPT-2 language model\n# Load pre-trained GPT-2 model\n#model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                                             torch_dtype=torch.bfloat16,).to(device)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Enable gradient checkpointing if you run into memory issues\n#model.gradient_checkpointing_enable()\n\n# %%\n# Print the model's architecture to inspect the names of the modules\n#print(model)\n\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:14:58.786964Z","iopub.execute_input":"2025-06-08T19:14:58.787192Z","iopub.status.idle":"2025-06-08T19:15:08.885550Z","shell.execute_reply.started":"2025-06-08T19:14:58.787176Z","shell.execute_reply":"2025-06-08T19:15:08.884745Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7287d9008f94301b54dd4ce251a8490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d8ca0d13e1e4dbb8e5c489c812e9be2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78203bb18059461e9ec5b22e161317e5"}},"metadata":{}},{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.33 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":9},{"id":"0863d39d","cell_type":"code","source":"# %%\n\n# Data collator for language modeling (masks tokens for prediction)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) # mlm=False for causal LM\n\n# Perplexity as metric\n#perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    shift_logits = torch.tensor(logits[:, :-1, :])\n    shift_labels = torch.tensor(labels[:, 1:])\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    ppl = torch.exp(loss).item()\n    return {\"perplexity\": ppl}\n\n# Training args\ntraining_args = TrainingArguments(\n    output_dir=\"./lama-dialog-finetuned\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=1,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    logging_steps=1000,\n    #evaluation_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    report_to=\"none\",\n    remove_unused_columns=False,\n    bf16=True,  # Enable bfloat16\n    fp16=False,  # Disable fp16 to avoid conflicts\n)\n# %%\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:08.886440Z","iopub.execute_input":"2025-06-08T19:15:08.886747Z","iopub.status.idle":"2025-06-08T19:15:08.929106Z","shell.execute_reply.started":"2025-06-08T19:15:08.886730Z","shell.execute_reply":"2025-06-08T19:15:08.928322Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 2.05 GB, Cached: 2.14 GB\n[CPU] Memory Usage: 2.34 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":10},{"id":"0355f85f","cell_type":"code","source":"class MemoryCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        print(\"\\nMemory footprint after evaluation:\")\n        print_memory_footprint()\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    #eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    #compute_metrics=compute_metrics,\n    callbacks=[MemoryCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:08.930051Z","iopub.execute_input":"2025-06-08T19:15:08.930681Z","iopub.status.idle":"2025-06-08T19:15:09.191834Z","shell.execute_reply.started":"2025-06-08T19:15:08.930658Z","shell.execute_reply":"2025-06-08T19:15:09.191185Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3187942852.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":11},{"id":"5daa30a3","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\nmodel_before_finetune = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n             torch_dtype=torch.bfloat16).to(device)\n\ninput_text_before = \"Hello, how are you?\"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:09.192949Z","iopub.execute_input":"2025-06-08T19:15:09.193532Z","iopub.status.idle":"2025-06-08T19:15:12.482091Z","shell.execute_reply.started":"2025-06-08T19:15:09.193514Z","shell.execute_reply":"2025-06-08T19:15:12.481096Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response (Before): Hello, how are you?\nI am fine, thank you.\nHow are you? I am fine, thank you.\nCan you repeat that again?\nCan you repeat that again, please?\nCan you repeat that again,\n---\n","output_type":"stream"}],"execution_count":12},{"id":"b21b2ae4","cell_type":"code","source":"\nprint(\"--- TinyLlama Before Fine-tuning ---\")\n#model_before_finetune = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\ninput_text_before = \"The USA celebrate independence day on \"\ninput_ids_before = tokenizer.encode(input_text_before, return_tensors=\"pt\").to(device)\n\nprint(f\"Input: {input_text_before}\")\noutput_before = model_before_finetune.generate(\n    input_ids_before,\n    max_length=50,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\ngenerated_response_before = tokenizer.decode(output_before[0], skip_special_tokens=True)\nprint(f\"Generated Response (Before): {generated_response_before}\")\nprint(\"---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:12.483002Z","iopub.execute_input":"2025-06-08T19:15:12.483263Z","iopub.status.idle":"2025-06-08T19:15:13.938263Z","shell.execute_reply.started":"2025-06-08T19:15:12.483245Z","shell.execute_reply":"2025-06-08T19:15:13.937363Z"}},"outputs":[{"name":"stdout","text":"--- TinyLlama Before Fine-tuning ---\nInput: The USA celebrate independence day on \nGenerated Response (Before): The USA celebrate independence day on 4th July.\n\n2. The USA celebrate independence day on 4th July.\n\n3. The USA celebrate independence day on 4th July.\n\n4. The\n---\n","output_type":"stream"}],"execution_count":13},{"id":"9cb4c8f9","cell_type":"code","source":"any(p.requires_grad for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:13.939344Z","iopub.execute_input":"2025-06-08T19:15:13.939647Z","iopub.status.idle":"2025-06-08T19:15:13.946253Z","shell.execute_reply.started":"2025-06-08T19:15:13.939621Z","shell.execute_reply":"2025-06-08T19:15:13.945374Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"id":"16e84bb6","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:15:13.946834Z","iopub.execute_input":"2025-06-08T19:15:13.947088Z","iopub.status.idle":"2025-06-08T19:25:16.047256Z","shell.execute_reply.started":"2025-06-08T19:15:13.947069Z","shell.execute_reply":"2025-06-08T19:25:16.046535Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 09:56, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=0.7564185180664063, metrics={'train_runtime': 599.1388, 'train_samples_per_second': 3.338, 'train_steps_per_second': 0.835, 'total_flos': 3150837652340736.0, 'train_loss': 0.7564185180664063, 'epoch': 1.0})"},"metadata":{}}],"execution_count":15},{"id":"17b74a82-ad5d-42cf-afe1-cd84034ecabb","cell_type":"code","source":"trainer.save_model('TinyLlama-new-1000')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:25:16.048008Z","iopub.execute_input":"2025-06-08T19:25:16.048294Z","iopub.status.idle":"2025-06-08T19:25:16.215946Z","shell.execute_reply.started":"2025-06-08T19:25:16.048271Z","shell.execute_reply":"2025-06-08T19:25:16.215443Z"}},"outputs":[],"execution_count":16},{"id":"13a12b7c-3d5a-455e-a483-1fbc1c954668","cell_type":"code","source":"print_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:25:16.216575Z","iopub.execute_input":"2025-06-08T19:25:16.216781Z","iopub.status.idle":"2025-06-08T19:25:16.221214Z","shell.execute_reply.started":"2025-06-08T19:25:16.216766Z","shell.execute_reply":"2025-06-08T19:25:16.220555Z"}},"outputs":[{"name":"stdout","text":"[GPU] Memory Allocated: 4.12 GB, Cached: 9.63 GB\n[CPU] Memory Usage: 3.08 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":17},{"id":"98331efb-c1a1-4223-b183-5731ddbddd78","cell_type":"code","source":"# Dynamic device assignment\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# For generation, we'll load the model and use the `generate` method\nmodel_for_generation = AutoModelForCausalLM.from_pretrained('TinyLlama-new-1000').to(device)\n#model_for_generation = get_peft_model(model_for_generation, lora_config) # Ensure LoRA is applied\n\ninput_text = \"Hello, how are you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")\n\nprint_memory_footprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:25:16.221980Z","iopub.execute_input":"2025-06-08T19:25:16.222320Z","iopub.status.idle":"2025-06-08T19:25:23.465801Z","shell.execute_reply.started":"2025-06-08T19:25:16.222298Z","shell.execute_reply":"2025-06-08T19:25:23.465184Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nGenerating response:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input: Hello, how are you?\nGenerated Response: Hello, how are you? I'm good, thanks. How about you? I'm doing well. I'm glad to hear that. It's good to hear that you're doing well. That's great to hear. I'm glad to hear that. It's always good to hear that someone is doing well. That's great to hear. I'm glad to hear that. It's always good to hear that someone is doing well. That's great to hear. I'm glad to hear that. It's always good to hear that someone is doing well. That's great to hear. I'm glad to hear that. It's always good to\n[GPU] Memory Allocated: 8.22 GB, Cached: 10.63 GB\n[CPU] Memory Usage: 3.87 GB / 31.35 GB\n","output_type":"stream"}],"execution_count":18},{"id":"e8dbebe8-310c-420d-a21e-95c3ea707bb2","cell_type":"code","source":"input_text = \"The USA celebrate independence day on \"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=150,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:25:23.466534Z","iopub.execute_input":"2025-06-08T19:25:23.466771Z","iopub.status.idle":"2025-06-08T19:25:27.203898Z","shell.execute_reply.started":"2025-06-08T19:25:23.466754Z","shell.execute_reply":"2025-06-08T19:25:27.203317Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: The USA celebrate independence day on \nGenerated Response: The USA celebrate independence day on 4th July.\n\n2. India Independence Day is celebrated on 15th August.\n\n3. Canada celebrates its 150th birthday on July 1st.\n\n4. Australia celebrates its 238th birthday on 26th January.\n\n5. New Zealand celebrates its 150th birthday on 28th September.\n\n6. South Africa celebrates its 21st democratic election on 27th April.\n\n7. Ghana celebrates its 60th independence day on 6th July.\n\n8. Nigeria celebrates its\n","output_type":"stream"}],"execution_count":19},{"id":"6281e296-a328-4f43-a4fa-67ff1c06c6b9","cell_type":"code","source":"input_text = \"Explain how gravity works?. Can you?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nprint(\"\\nGenerating response:\")\noutput = model_for_generation.generate(\n    input_ids,\n    max_length=250,\n    num_return_sequences=1,\n    temperature=0.7,\n    pad_token_id=tokenizer.eos_token_id\n)\n\ngenerated_response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f\"Input: {input_text}\")\nprint(f\"Generated Response: {generated_response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T19:33:28.578007Z","iopub.execute_input":"2025-06-08T19:33:28.578509Z","iopub.status.idle":"2025-06-08T19:33:28.635269Z","shell.execute_reply.started":"2025-06-08T19:33:28.578489Z","shell.execute_reply":"2025-06-08T19:33:28.634678Z"}},"outputs":[{"name":"stdout","text":"\nGenerating response:\nInput: Explain how gravity works?\nGenerated Response: Explain how gravity works?\n","output_type":"stream"}],"execution_count":21}]}